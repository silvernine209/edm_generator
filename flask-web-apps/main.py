import os
#import magic
import urllib.request
from app import app
import flask
from flask import Flask, flash, request, redirect, render_template
from werkzeug.utils import secure_filename
import pandas as pd
import numpy as np
import pickle
import os
import librosa
import matplotlib.pyplot as plt
import io
import base64
import seaborn as sns
from graph import build_graph
from music21 import converter, instrument, note, chord,stream
# Music Related
import IPython.display as ipd
import pypianoroll
from music21 import converter, instrument, note, chord,stream
# Modeling
from keras.layers import LSTM,Dropout,Dense,Flatten,BatchNormalization,Activation
from keras.models import Sequential
from keras.utils.np_utils import to_categorical
from keras_radam import RAdam
from sklearn.preprocessing import MultiLabelBinarizer,LabelEncoder, StandardScaler
import tensorflow as tf

# Don't always choose the most likely prediction
def sample(preds, temperature):
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds) / temperature
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    probas = np.random.multinomial(1, preds, 1)
    return np.argmax(probas)



def generate_melody(temperature,song_length):
    # Classic 6 semi
    from music21 import converter, instrument, note, chord, stream
    model_weight = 'classic_3steps_6semitone_adj02-1.0370-bigger.hdf5'
    notes = pickle.load(open("notes_semitone_6_classic.pkl", "rb"))

    sequence_length = 3
    n_vocab = len(list(set(notes)))
    # get all pitch names
    pitchnames = sorted(set(item for item in notes))
    # create a dictionary to map pitches to integers
    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))
    network_input = []
    network_output = []
    # create input sequences and the corresponding outputs
    for i in range(0, len(notes) - sequence_length, 1):
        sequence_in = notes[i:i + sequence_length]
        sequence_out = notes[i + sequence_length]
        network_input.append([note_to_int[char] for char in sequence_in])
        network_output.append(note_to_int[sequence_out])
    n_patterns = len(network_input)
    # reshape the input into a format compatible with LSTM layers
    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))
    # normalize input
    network_input = network_input / float(n_vocab)
    network_output = to_categorical(network_output)

    # 12 semitone
    model = Sequential()
    model.add(LSTM(
        512,
        input_shape=(network_input.shape[1], network_input.shape[2]),
        return_sequences=True
    ))
    model.add(Dropout(0.3))
    model.add(LSTM(512, return_sequences=True))
    model.add(Dropout(0.3))
    model.add(LSTM(512))
    model.add(Dense(256))
    model.add(Dropout(0.3))
    model.add(Dense(n_vocab))
    model.add(Activation('softmax'))
    model.compile(RAdam(), loss='categorical_crossentropy')

    # Load the weights
    model.load_weights(model_weight)



    # Generate new song
    start = np.random.randint(0, len(network_input)-1)
    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))
    pattern = list(network_input[start])
    prediction_output = []

    # generate 300 notes
    # temperatures = [0.9,1.1,1.3,1.8,2]
    # temperatures = [x+3 for x in temperatures]
    temperatures = [temperature]
    melody_length = song_length


    for note_index in range(melody_length):
        prediction_input = np.reshape(pattern, (1, len(pattern), 1))
        prediction_input = prediction_input / float(n_vocab)
        with graph1.as_default():
            prediction = model.predict(prediction_input)
        #index = np.argmax(prediction)
        #temperature = np.random.choice(temperatures,p=[0.7,0.15,0.1,0.04,0.01])
        index = sample(prediction[0],temperatures)
        result = int_to_note[index]
        prediction_output.append(result)
        pattern.append(index)
        pattern = pattern[1:len(pattern)]

    offset = 0
    output_notes = []
    # create note and chord objects based on the values generated by the model
    for pattern in prediction_output:
        # pattern is a chord
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:
                new_note = note.Note(int(current_note))
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
        # pattern is a note
        else:
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)
        # increase offset each iteration so that notes do not stack
        offsets = [0.4,0.8]
        offset += np.random.choice(offsets,p=[0.98,0.02])
        #ofset += 0.35
    # Save midi file
    midi_stream = stream.Stream(output_notes)
    midi_stream.write('midi', fp='melody_output.mid'.format(temperature,song_length))

    from music21 import converter, instrument, note, chord,stream
    model_weight = '15_notes_drum2884-0.0759-bigger.hdf5'

    notes_drum = pickle.load( open( "drum.pkl", "rb" ) )

    sequence_length = 15
    n_vocab = len(list(set(notes_drum)))
    # get all pitch names
    pitchnames = sorted(set(item for item in notes_drum))
    # create a dictionary to map pitches to integers
    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))
    network_input = []
    network_output = []
    # create input sequences and the corresponding outputs
    for i in range(0, len(notes_drum) - sequence_length, 1):
        sequence_in = notes_drum[i:i + sequence_length]
        sequence_out = notes_drum[i + sequence_length]
        network_input.append([note_to_int[char] for char in sequence_in])
        network_output.append(note_to_int[sequence_out])
    n_patterns = len(network_input)
    # reshape the input into a format compatible with LSTM layers
    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))
    # normalize input
    network_input = network_input / float(n_vocab)
    network_output = to_categorical(network_output)

    # 12 semitone
    model = Sequential()
    model.add(LSTM(
        256,
        input_shape=(network_input.shape[1], network_input.shape[2]),
        return_sequences=True
    ))
    model.add(Dropout(0.3))
    model.add(LSTM(512, return_sequences=True))
    model.add(Dropout(0.3))
    model.add(LSTM(256))
    model.add(Dense(256))
    model.add(Dropout(0.3))
    model.add(Dense(n_vocab))
    model.add(Activation('softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam')

    # Load the weights
    model.load_weights(model_weight)

    # Generate new song
    start = np.random.randint(0, len(network_input)-1)
    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))
    pattern = list(network_input[start])
    prediction_output = []
    # generate 300 notes
    temperatures = [0.4,0.8,1.1,1.5,2]
    temperatures = [x+0.3 for x in temperatures]


    for note_index in range(song_length):
        prediction_input = np.reshape(pattern, (1, len(pattern), 1))
        prediction_input = prediction_input / float(n_vocab)
        with graph2.as_default():
            prediction = model.predict(prediction_input)
        #index = np.argmax(prediction)
        index = sample(prediction[0],np.random.choice(temperatures,p=[0.6,0.15,0.11,0.09,0.05]))
        result = int_to_note[index]
        prediction_output.append(result)
        pattern.append(index)
        pattern = pattern[1:len(pattern)]

    offset = 0
    output_notes = []
    # create note and chord objects based on the values generated by the model
    for pattern in prediction_output:
        # pattern is a chord
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:
                new_note = note.Note(int(current_note))
                new_note.storedInstrument = instrument.BassDrum()
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
        # pattern is a note
        else:
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.BassDrum()
            output_notes.append(new_note)
        # increase offset each iteration so that notes do not stack
        offsets = [0.8,0.4]
        offset += np.random.choice(offsets,p=[0.7,0.3])
    # Save midi file
    midi_stream = stream.Stream(output_notes)
    midi_stream.write('midi', fp='drum_output.mid')

    # Play the song for semitone
    test_midi = pypianoroll.Multitrack('drum_output.mid')
    drum_midi = pypianoroll.Multitrack('blank.mid')
    drum_midi.tempo = drum_midi.tempo/drum_midi.tempo*120
    drum_midi.tracks[2].pianoroll = np.roll(test_midi.tracks[0].pianoroll,0)
    drum_midi.tracks.pop(0)
    drum_midi.tracks.pop(0)

    pypianoroll.write(drum_midi,'drum_output.mid')

    # Play the song for semitone :
    melody_midi = pypianoroll.Multitrack('melody_output.mid')
    drum_midi = pypianoroll.Multitrack('drum_output.mid')

    blank_midi =  pypianoroll.Multitrack('blank.mid')
    blank_midi.tempo = blank_midi.tempo/blank_midi.tempo*115

    blank_midi.tracks[0].pianoroll = np.roll(melody_midi.tracks[0].pianoroll[:],0)[200:]
    number = 0
    blank_midi.tracks[0].program = number
    #blank_midi.tracks[1].pianoroll =  #Bass
    blank_midi.tracks[2].pianoroll = drum_midi.tracks[0].pianoroll[:len(blank_midi.tracks[0].pianoroll)] #Cap drum length to melody's

    blank_midi.tracks.pop(1) #Pop bass for now

    pypianoroll.write(blank_midi,'midi/user_output_{}_{}.mid'.format(temperature,song_length))

@app.route("/")
def generate_post():
    return flask.render_template('generate.html')

@app.route("/", methods=["POST"])
def generate_post_post():
    global graph1
    global graph2
    graph1 = tf.get_default_graph()
    graph2 = tf.get_default_graph()
    if request.method == "POST":
        temperature = float(request.form['temperature'])
        melody_length = int(request.form['melody_length'])
        generate_melody(temperature, melody_length)
    return flask.render_template('generate.html')


@app.route('/midiplayer', methods=["POST", "GET"])
def graphs():
    return render_template('midiplayer.html')#,file1 = )


if __name__ == "__main__":
	app.run()
